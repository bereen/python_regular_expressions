{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `BERT`\n",
    "**`BERT`** (от англ. <i>Bidirectional Encoder Representations from Transformers, «двунаправленная нейронная сеть-кодировщик»</i>) — нейронная сеть для создания модели языка. Её разработали в компании $Google$, чтобы повысить релевантность результатов поиска. Этот алгоритм понимает контекст запросов, а не просто анализирует фразы. Для машинного обучения она ценна тем, что помогает строить векторные представления. Причём в анализе текстов применяют уже предобученную на большом корпусе модель. Такие предобученные версии BERT годятся для работы с текстами на 104 языках мира, включая русский.\n",
    "<br>**`BERT`** — это результат эволюции модели `word2vec`. В ходе её развития были придуманы и другие модели: `FastText` (англ. «быстрый текст»), `GloVe` (англ. <i>Global Vectors for Word Representation, «глобальные векторы для языкового представления»</i>), `ELMO` (англ. <i>Embeddings from Language Models, «вложения языковых моделей»</i>) и `GPT` (англ. <i>Generative Pre-Training Transformer, «предобученный трансформер для генерации»</i>). Сейчас самые точные — это `BERT` и `GPT-3`, которого нет в открытом доступе.\n",
    "<br>`BERT` учитывает контекст не только соседних слов, но и более дальних родственников. Работает так:\n",
    "- На входе модель получает, например, такую фразу:\n",
    "<br>«Красный клюв тупика [MASK] на голубом [MASK]», где MASK (англ. «маска») это неизвестные слова, будто закрытые маской. Модель должна угадать эти спрятанные слова.\n",
    "- Модель обучается определять, связаны ли в предложении слова между собой. У нас были скрыты такие слова:\n",
    "<br>«мелькнул» и «небе». Модель должна понять, что одно слово — продолжение другого. Скажем, если вместо «мелькнул» спрятать слово «прополз», то связи модель не найдёт.\n",
    "# `RuBERT` и предобработка\n",
    "Построить векторы текстов нам поможет предобученная на русских текстах модель RuBERT.\n",
    "# Задача\n",
    "Перед вами большой датасет с твитами. Нужно научиться определять, какие твиты негативной тональности, а какие — позитивной. Чтобы решить эту задачу, из открытого репозитория [`DeepPavlov`](https://docs.deeppavlov.ai/en/master/features/models/bert.html) возьмём модель `RuBERT`, обученную на разговорном русскоязычном корпусе. \n",
    "<br>Решим эту задачу на `PyTorch` (англ. <i>«факел для Python»</i>). Hужна для работы с моделями типа `BERT`. Они находятся в библиотеке `transformers` (англ. <i>«трансформеры»</i>). Импортируем их:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp37-cp37m-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in d:\\programs\\lib\\site-packages (from transformers) (1.5.0)\n",
      "Collecting huggingface-hub<1.0,>=0.14.1\n",
      "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in d:\\programs\\lib\\site-packages (from transformers) (4.42.1)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programs\\lib\\site-packages (from transformers) (1.18.1)\n",
      "Requirement already satisfied: filelock in d:\\programs\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Collecting regex!=2019.12.17\n",
      "  Downloading regex-2024.4.16-cp37-cp37m-win_amd64.whl (269 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programs\\lib\\site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: requests in d:\\programs\\lib\\site-packages (from transformers) (2.22.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programs\\lib\\site-packages (from transformers) (5.3)\n",
      "Collecting safetensors>=0.3.1\n",
      "  Downloading safetensors-0.4.3-cp37-none-win_amd64.whl (287 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\programs\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (2.2.0)\n",
      "Requirement already satisfied: fsspec in d:\\programs\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (0.6.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programs\\lib\\site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\programs\\lib\\site-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\programs\\lib\\site-packages (from requests->transformers) (1.25.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programs\\lib\\site-packages (from requests->transformers) (2019.11.28)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\programs\\lib\\site-packages (from requests->transformers) (2.8)\n",
      "Installing collected packages: tokenizers, huggingface-hub, regex, safetensors, transformers\n",
      "Successfully installed huggingface-hub-0.16.4 regex-2024.4.16 safetensors-0.4.3 tokenizers-0.13.3 transformers-4.30.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\programs\\lib\\site-packages (from evaluate) (2.22.0)\n",
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.15-py37-none-any.whl (116 kB)\n",
      "Requirement already satisfied: packaging in d:\\programs\\lib\\site-packages (from evaluate) (23.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-3.4.1-cp37-cp37m-win_amd64.whl (29 kB)\n",
      "Collecting datasets>=2.0.0\n",
      "  Downloading datasets-2.13.2-py3-none-any.whl (512 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in d:\\programs\\lib\\site-packages (from evaluate) (0.16.4)\n",
      "Collecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2023.1.0-py3-none-any.whl (143 kB)\n",
      "Requirement already satisfied: pandas in d:\\programs\\lib\\site-packages (from evaluate) (1.0.1)\n",
      "Collecting dill\n",
      "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
      "Collecting responses<0.19\n",
      "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.66.4-py3-none-any.whl (78 kB)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in d:\\programs\\lib\\site-packages (from evaluate) (1.5.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\programs\\lib\\site-packages (from evaluate) (1.18.1)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in d:\\programs\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programs\\lib\\site-packages (from requests>=2.19.0->evaluate) (2019.11.28)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in d:\\programs\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in d:\\programs\\lib\\site-packages (from requests>=2.19.0->evaluate) (1.25.8)\n",
      "Collecting aiohttp\n",
      "  Downloading aiohttp-3.8.6-cp37-cp37m-win_amd64.whl (326 kB)\n",
      "Collecting pyarrow>=8.0.0\n",
      "  Downloading pyarrow-12.0.1-cp37-cp37m-win_amd64.whl (21.5 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\programs\\lib\\site-packages (from datasets>=2.0.0->evaluate) (5.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\programs\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.5.0)\n",
      "Requirement already satisfied: filelock in d:\\programs\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (3.0.12)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in d:\\programs\\lib\\site-packages (from pandas->evaluate) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.2 in d:\\programs\\lib\\site-packages (from pandas->evaluate) (2022.7.1)\n",
      "Requirement already satisfied: colorama; platform_system == \"Windows\" in d:\\programs\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\programs\\lib\\site-packages (from importlib-metadata; python_version < \"3.8\"->evaluate) (2.2.0)\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\programs\\lib\\site-packages (from aiohttp->datasets>=2.0.0->evaluate) (19.3.0)\n",
      "Collecting asynctest==0.13.0; python_version < \"3.8\"\n",
      "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp37-cp37m-win_amd64.whl (77 kB)\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.3.3-cp37-cp37m-win_amd64.whl (34 kB)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.5-cp37-cp37m-win_amd64.whl (26 kB)\n",
      "Collecting charset-normalizer<4.0,>=2.0\n",
      "  Downloading charset_normalizer-3.3.2-cp37-cp37m-win_amd64.whl (98 kB)\n",
      "Collecting async-timeout<5.0,>=4.0.0a3\n",
      "  Downloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programs\\lib\\site-packages (from python-dateutil>=2.6.1->pandas->evaluate) (1.14.0)\n",
      "Installing collected packages: dill, multiprocess, xxhash, frozenlist, aiosignal, asynctest, multidict, yarl, charset-normalizer, async-timeout, aiohttp, fsspec, pyarrow, tqdm, datasets, responses, evaluate\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 0.6.2\n",
      "    Uninstalling fsspec-0.6.2:\n",
      "      Successfully uninstalled fsspec-0.6.2\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.42.1\n",
      "    Uninstalling tqdm-4.42.1:\n",
      "      Successfully uninstalled tqdm-4.42.1\n",
      "Successfully installed aiohttp-3.8.6 aiosignal-1.3.1 async-timeout-4.0.3 asynctest-0.13.0 charset-normalizer-3.3.2 datasets-2.13.2 dill-0.3.7 evaluate-0.4.1 frozenlist-1.3.3 fsspec-2023.1.0 multidict-6.0.5 multiprocess-0.70.15 pyarrow-12.0.1 responses-0.18.0 tqdm-4.66.4 xxhash-3.4.1 yarl-1.9.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: conda-build 3.18.11 requires conda, which is not installed.\n",
      "ERROR: datasets 2.13.2 has requirement dill<0.3.7,>=0.3.0, but you'll have dill 0.3.7 which is incompatible.\n",
      "ERROR: responses 0.18.0 has requirement urllib3>=1.25.10, but you'll have urllib3 1.25.8 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "%pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем перевести тексты в векторы, подготовим их. У RuBERT есть собственный токенизатор. Это инструмент, который разбивает и преобразует исходные тексты в список токенов, которые есть, например, в словаре RuBERT. Лемматизация не требуется.\n",
    "<br>Начинаем предобработку текстов:\n",
    "1. Инициализируем токенизатор как объект класса `BertTokenizer()`. Передадим ему аргумент `vocab_file` — это файл со словарём, на котором обучалась модель. Он может быть, например, в текстовом формате ($txt$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = transformers.BertTokenizer(\n",
    "    vocab_file='tweets_lemm_test.csv')#'/datasets/ds_bert/vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Преобразуем текст в номера токенов из словаря методом `encode()` (англ. <i>«закодировать»</i>):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode('Очень удобно использовать уже готовый трансформатор текста', add_special_tokens=True)\n",
    "\n",
    "\n",
    "яндекс на*бывает, херь не работает, падает в ошибку, типа не тензор передан, они *ёбки дикие"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для корректной работы модели мы указали аргумент `add_special_tokens` (англ. <i>«добавить специальные токены»</i>), равный $True$. Это значит, что к любому преобразуемому тексту добавляется токен начала (101) и токен конца текста (102). \n",
    "3. Применим метод `padding` (англ. <i>«отступ»</i>), чтобы после токенизации длины исходных текстов в корпусе были равными. Только при таком условии будет работать модель `BERT`. Пусть стандартной длиной вектора $n$ будет длина наибольшего во всём датасете вектора. Остальные векторы дополним нулями:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = tokenizer.encode('Очень удобно использовать уже готовый трансформатор текста', add_special_tokens=True)\n",
    "n = 280\n",
    "# англ. вектор с отступами\n",
    "padded = np.array(vector + [0]*(n - len(vector)))\n",
    "print(padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь поясним модели, что нули не несут значимой информации. Это нужно для компоненты модели, которая называется «внимание» (англ. <i>attention</i>). Отбросим эти токены и «создадим маску» для действительно важных токенов, то есть укажем нулевые и не нулевые значения:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "print(attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Эмбеддинги RuBERT\n",
    "Инициализируем конфигурацию `BertConfig` (англ. <i>Bert Configuration</i>). В качестве аргумента передадим ей JSON-файл с описанием настроек модели. `JSON` (англ. <i>JavaScript Object Notation, «объектная запись JavaScript»</i>) — это организованный по ключам поток цифр, букв, двоеточий и фигурных скобок, который возвращает сервер при запросе. \n",
    "Затем инициализируем саму модель класса `BertModel`. Передадим ей файл с предобученной моделью и конфигурацией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = transformers.BertConfig.from_json_file(\n",
    "    '/datasets/ds_bert/bert_config.json')\n",
    "model = transformers.BertModel.from_pretrained(\n",
    "    '/datasets/ds_bert/rubert_model.bin', config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнём преобразование текстов в эмбеддинги. Это может занять несколько минут, поэтому подключим библиотеку `tqdm` (араб. <i>taqadum, تقدّم, «прогресс»</i>). Она нужна, чтобы наглядно показать индикатор прогресса. В Jupyter применим функцию `notebook()` из этой библиотеки: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддинги модель `BERT` создаёт батчами. Чтобы хватило оперативной памяти, сделаем размер батча небольшим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем цикл по батчам. Отображать прогресс будет функция `notebook()`\n",
    "<br>01) Преобразуем данные в формат тензоров (англ. <i>tensor</i>) — многомерных векторов в библиотеке `torch`. Тип данных `LongTensor` (англ. <i>«длинный тензор»</i>) хранит числа в «длинном формате», то есть выделяет на каждое число 64 бита.:\n",
    "<br>02) Чтобы получить эмбеддинги для батча, передадим модели данные и маску:\n",
    "<br>03) Для ускорения вычисления функцией `no_grad()` (англ. <i>no gradient, «нет градиента»</i>) в библиотеке torch укажем, что градиенты не нужны: модель `BERT` обучать не будем.\n",
    "<br>04) Из полученного тензора извлечём нужные элементы и добавим в список всех эмбеддингов:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделаем пустой список для хранения эмбеддингов твитов\n",
    "embeddings = []\n",
    "\n",
    "for i in notebook.tqdm(range(padded.shape[0] // batch_size)):\n",
    "    #01) преобразуем данные\n",
    "    batch = torch.LongTensor(padded[batch_size*i:batch_size*(i+1)])\n",
    "    # 02)преобразуем маску\n",
    "    attention_mask_batch = torch.LongTensor(attention_mask[batch_size*i:batch_size*(i+1)])\n",
    "#     02) получить эмбеддинги для батча\n",
    "    batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "#     03) no_grad()\n",
    "    with torch.no_grad():\n",
    "        batch_embeddings = model(batch, attention_mask=attention_mask_batch)\n",
    "    #04) преобразуем элементы методом numpy() к типу numpy.array\n",
    "    embeddings.append(batch_embeddings[0][:,0,:].numpy()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Соберём все эмбеддинги в матрицу признаков вызовом функции `concatenate()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = np.concatenate(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
